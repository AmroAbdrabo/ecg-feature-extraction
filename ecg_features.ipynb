{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features are\n",
    "## 1) RR average length, mean and median QRS width, RR length standard deviation\n",
    "## 2) Number of changes in sign of the the slope in the R-R interval (per time)\n",
    "## 3) standard deviation of slope in the R-R interval as well as average and median of the absolute value of the slopes\n",
    "## 4) PCA on DWT\n",
    "## 5) Difference between median and mean heartbeats (the norm of the vector)\n",
    "## 6) std dev. of the pre-R part of the beat\n",
    "## 7) HOS descriptor\n",
    "## 8) k-means of RR lengths\n",
    "## 9) Autoregression coefficients of order 9s and 10: x(n)  = coeff1 * x(n-1) + coeff2 * x(n-2) +..+ ## coeff9*x(n-9) and x(n)  = coeff1 * x(n-1) + coeff2 * x(n-2) +..+ ## coeff10*x(n-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from pywt import wavedec\n",
    "from pywt import waverec\n",
    "from statsmodels.tsa.stattools import levinson_durbin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pywt\n",
    "import scipy\n",
    "#from noiseremove import func0\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from biosppy.signals import ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amroa/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (17979) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dfX = pd.read_csv(\"X_train.csv\").iloc[:, 1:]\n",
    "dfTest = pd.read_csv(\"X_test.csv\").iloc[:, 1:]\n",
    "dfY = pd.read_csv(\"y_train.csv\").iloc[:, 1:]\n",
    "XPred = dfTest.iloc[:, 1:].values\n",
    "y = dfY.values\n",
    "MAX_SENT = 10e5\n",
    "X = dfX.fillna(MAX_SENT).values  # fill the nan with sentinel values to prevent error later\n",
    "X[-1][-1] = MAX_SENT\n",
    "Xt= dfTest.fillna(MAX_SENT).values  # fill the nan with sentinel values to prevent error later\n",
    "Xt[-1][-1] = MAX_SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_good = np.where(y !=3)[0]\n",
    "X_ = X[y_good]\n",
    "y_ = y[y_good]  \n",
    "## undoes the previous statements (without having to change the code below)\n",
    "X_= X\n",
    "y_ = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'func0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-061380af1a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds3\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'func0' is not defined"
     ]
    }
   ],
   "source": [
    "#preds3 = func0().astype(int)\n",
    "#available = np.where(preds3 != 1)[0]\n",
    "#Xt = Xt[available]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxxl = -9 ==> 17979\n",
    "def filtersig(rsig):\n",
    "    padded = np.array([MAX_SENT]*17979)\n",
    "    sig_ = rsig[rsig < MAX_SENT]\n",
    "    info = ecg.ecg(signal=sig_, sampling_rate= 300, show=False)\n",
    "    filt_sig = info[1]\n",
    "    padded[0:len(filt_sig)] = filt_sig\n",
    "    return padded\n",
    "\n",
    "X_filt = np.apply_along_axis(filtersig, axis = 1, arr = X_)\n",
    "#slope_stats = np.apply_along_axis(slopestat, axis = 1, arr = mix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = -5\n",
    "def getRpeaks(sig):\n",
    "    #global max_len 160\n",
    "    padded = np.array([MAX_SENT]*212)\n",
    "    sig_ = sig[sig < MAX_SENT]\n",
    "    info = ecg.ecg(signal=sig_, sampling_rate= 300, show=False)\n",
    "    rpks= info[2]\n",
    "    padded[0:len(rpks)] = rpks\n",
    "    return padded\n",
    "\n",
    "r_peaks = np.apply_along_axis(getRpeaks, axis = 1, arr = X_)\n",
    "mix_data2 = np.concatenate((r_peaks, X_filt), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New: Generate new samples from existing ones\n",
    "The idea consists of taking two signals of the same class. We then split the two signals in half, and combine them again using the first half of signal A and the second half of signal B to get a new signal. However, to avoid messing up the QRS complexes morphology we use the mean RR interval of the common terminals of the original signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSigs(mix_1, mix_2):\n",
    "    \n",
    "    rpks1 = mix_1[0:212]\n",
    "    rpks2 = mix_2[0:212]\n",
    "    \n",
    "    rpks1  = np.array(rpks1[rpks1 < MAX_SENT], dtype = int)\n",
    "    rpks2  = np.array(rpks2[rpks2 < MAX_SENT], dtype = int)\n",
    "    \n",
    "    rlen1 = int(len(rpks1)/2)\n",
    "    rlen2 = int(np.floor(len(rpks2)/2)+1)\n",
    "    res = np.array([MAX_SENT]*17979)\n",
    "    \n",
    "    if (rlen1 < 2 or rlen2 < 2):\n",
    "        return res\n",
    "    \n",
    "    sig1 = mix_1[212:]\n",
    "    sig2 = mix_2[212:]\n",
    "    sig1  = sig1[sig1 < MAX_SENT]\n",
    "    sig2  = sig2[sig2 < MAX_SENT]\n",
    "    \n",
    "    mid1_pre = rpks1[rlen1]\n",
    "    mid1_post = rpks1[rlen1 + 1]\n",
    "    \n",
    "    mid2_post = rpks2[rlen2]\n",
    "    mid2_pre = rpks2[rlen2 - 1]\n",
    "    \n",
    "    mid1 = (mid1_pre + mid1_post)/2\n",
    "    mid2 = (mid2_pre + mid2_post)/2\n",
    "    \n",
    "    sig_fin = np.append(sig1[0:int(mid1)], sig2[int(mid2):])\n",
    "    res[0:len(sig_fin)] = sig_fin\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def sigincrement(sigclass, sig_idx_start, sig_idx_end):\n",
    "    global X_, y_\n",
    "    indices = np.where(y_ == sigclass)[0]\n",
    "    candidates = len(indices)\n",
    "    mix_data_cl = mix_data2[indices]   # mix_data2 is r_peaks, X_filt\n",
    "    for i in range(sig_idx_start, sig_idx_end):\n",
    "        first_sig = mix_data_cl[i]\n",
    "        for j in range(i+1, candidates):\n",
    "            second_sig = mix_data_cl[j]\n",
    "            new_sig = combineSigs(first_sig, second_sig)\n",
    "            X_ = np.vstack((X_, new_sig)) if (new_sig[0] < MAX_SENT) else X_\n",
    "            y_ = np.append(y_, sigclass) if (new_sig[0] < MAX_SENT) else y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4947, 17979)\n"
     ]
    }
   ],
   "source": [
    "print(X_.shape)\n",
    "sigincrement(1, 0, 2)\n",
    "print(X_.shape)\n",
    "\n",
    "# filtering and r-peak have to be redone to take into account the new samples\n",
    "X_filt = np.apply_along_axis(filtersig, axis = 1, arr = X_)\n",
    "r_peaks = np.apply_along_axis(getRpeaks, axis = 1, arr = X_)\n",
    "mix_data2 = np.concatenate((r_peaks, X_filt), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## newly added part ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "min_dist = 1000\n",
    "max_dist = -5\n",
    "def getRRstats(rpks):\n",
    "    #global min_dist, max_dist 1.0, 4050\n",
    "    rpks = rpks[rpks < MAX_SENT]\n",
    "    diff_r = np.ediff1d(rpks)\n",
    "    diff_r  = diff_r[diff_r < 600] # otherwise heart rate less than 40 bpm, 30 bpm -> 600\n",
    "    #diff_r  = diff_r[diff_r > 60] # otw hr greater than 200 bpm, 81 would correpond to 220 bpm\n",
    "    #min_dist = min(np.min(diff_r), min_dist)\n",
    "    #max_dist = max(np.max(diff_r), max_dist)\n",
    "    if (len(diff_r) == 0):\n",
    "        diff_r = np.array([250])\n",
    "    sigma = np.std(diff_r)\n",
    "    med  = np.median(diff_r)\n",
    "    avg = np.mean(diff_r)\n",
    "    return np.array([sigma, med, avg])\n",
    "    \n",
    "rr_stats = np.apply_along_axis(getRRstats, axis = 1, arr = r_peaks) \n",
    "print(max_dist)\n",
    "print(min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQRSwidth(mix_sig):\n",
    "    rpks = mix_sig[0:212]\n",
    "    sig = mix_sig[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    sig  = sig[sig < MAX_SENT]\n",
    "    \n",
    "    info = ecg.extract_heartbeats(signal=sig, rpeaks=rpks, sampling_rate= 300, before=0.30, after=0.30)\n",
    "    beats = info[0]\n",
    "    rpks = info[1]\n",
    "    rrints = []\n",
    "    for idx, bt in enumerate(beats):\n",
    "        left =  88\n",
    "        right = 92\n",
    "        while ((left > 0) and bt[left] < bt[left + 1]):\n",
    "            left -= 1\n",
    "        while ((right < (len(bt)-1)) and bt[right] > bt[right +1]):\n",
    "            right += 1\n",
    "        rrints.append(right - left)\n",
    "    res = np.array([np.mean(rrints), np.median(rrints)])\n",
    "    return res\n",
    "    ## go down to left and down to right until until hit bottom\n",
    "    ## get the mean and median across all beats\n",
    "mix_data = np.concatenate((r_peaks, X_), axis = 1)\n",
    "qrs_stats = np.apply_along_axis(getQRSwidth, axis = 1, arr =mix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slopestat(mix_data2):\n",
    "    ## inside the R-R interval look at the slope values\n",
    "    ## skip the upward and donward slopes by cutting off 20 samples from the extremities\n",
    "    ## get the slopes standard deviation of slope for each RR interval and average across all intervals\n",
    "    ## get the number of times the sign of the slope changes for each R-R intevral divided by amount of time\n",
    "    rpks = mix_data2[0:212]\n",
    "    sig = mix_data2[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    sig  = sig[sig < MAX_SENT]\n",
    "    if (len(rpks) <= 1): \n",
    "        return np.array([0]*6)\n",
    "    stddevs = []\n",
    "    slch = []\n",
    "    abs_sl = np.array([])\n",
    "    for i in range(len(rpks)-1):\n",
    "        peak1 = rpks[i]\n",
    "        peak2 = rpks[i+1]\n",
    "        if (peak2 - peak1 < 40):\n",
    "            continue\n",
    "        loff = 18 + peak1\n",
    "        roff = peak2 - 18\n",
    "        seg = sig[loff:roff]\n",
    "        slops = np.ediff1d(seg)\n",
    "        stddevs.append(np.std(slops))\n",
    "        abs_sl = np.append(abs_sl, np.absolute(slops))\n",
    "        #nbr times slopes changes divided by seg len\n",
    "        cnt = (np.diff(np.sign(slops)) != 0)*1\n",
    "        cnt_ = float(len(cnt[cnt > 0]))\n",
    "        slch.append((100*cnt_)/len(seg))\n",
    "    \n",
    "    res =  np.array([np.mean(slch), np.median(slch), np.mean(stddevs),np.median(stddevs), np.mean(abs_sl), np.median(abs_sl)])\n",
    "    return res\n",
    "    \n",
    "slop_feats = np.apply_along_axis(slopestat, axis = 1, arr = mix_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wavelet_descriptor(beat, family, level):\n",
    "    wave_family = pywt.Wavelet(family)\n",
    "    coeffs = pywt.wavedec(beat, family, level=level)\n",
    "    return coeffs[0]\n",
    "\n",
    "def DWT_beat(mix_sig):\n",
    "    rpks = mix_sig[0:212]\n",
    "    sig = mix_sig[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    sig  = sig[sig < MAX_SENT]\n",
    "    \n",
    "    info = ecg.extract_heartbeats(signal=sig, rpeaks=rpks, sampling_rate= 300, before=0.30, after=0.30)\n",
    "    \n",
    "    beat_pre = info[0]\n",
    "    bts_cnt = min(15, len(beat_pre))\n",
    "    wv = np.zeros((1, 23))[0]\n",
    "    for i in range(bts_cnt):\n",
    "        wv = np.add(wv, compute_wavelet_descriptor(beat_pre[i], 'db1', 3))\n",
    "    wv = wv/bts_cnt\n",
    "    return wv\n",
    "\n",
    "dwt_feats = np.apply_along_axis(DWT_beat, axis = 1, arr = mix_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_PCA(X, variance):\n",
    "    pca_ = PCA(variance)\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X)\n",
    "    Xsc = sc.transform(X)\n",
    "    pca_.fit(Xsc)\n",
    "    X_ = pca_.transform(Xsc)\n",
    "    return (X_, pca_, sc)\n",
    "    ## do pca with the given variance\n",
    "    ## return the transformed X as well as the PCA transformer (X, pca_variable)\n",
    "    \n",
    "Xtr, p_var, sc = trans_PCA(dwt_feats, 0.90) # try changing to 0.95 perhaps later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devpreR(mix_sig):   ## added post R also\n",
    "    rpks = mix_sig[0:212]\n",
    "    sig = mix_sig[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    sig  = sig[sig < MAX_SENT]\n",
    "    \n",
    "    # previously was 0.30 - 0.0\n",
    "    info = ecg.extract_heartbeats(signal=sig, rpeaks=rpks, sampling_rate= 300, before=0.30, after=0.4)\n",
    "    tmpls = info[0]\n",
    "    width = len(tmpls[0])\n",
    "    tmpls = tmpls[:, 0:int(3*width/7)]\n",
    "    nbrbt = len(tmpls)\n",
    "    matr = np.array([[0]*60]*nbrbt)  # 60 inst. of 55 could also work\n",
    "    for idx, bt in enumerate(tmpls):\n",
    "        matr[idx] = bt[0:60]\n",
    "    std60 = np.std(matr, axis= 0)\n",
    "    \n",
    "    tmpls2 = info[0]\n",
    "    tmpls2 = tmpls2[:, int(3*width/7):]\n",
    "    nbrbt = len(tmpls2)\n",
    "    width = len(tmpls2[0])/2\n",
    "    matr = np.array([[0]*60]*nbrbt)  # 60 inst. of 55 could also work\n",
    "    for idx, bt in enumerate(tmpls2):\n",
    "        matr[idx] = (bt[(int(width)-1):])[0:60]\n",
    "    stdpost = np.std(matr, axis= 0)\n",
    "    return np.append(std60, stdpost)\n",
    "\n",
    "devvec = np.apply_along_axis(devpreR, axis = 1, arr = mix_data2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the HOS descriptor for a beat\n",
    "# Skewness (3 cumulant) and kurtosis (4 cumulant)\n",
    "def compute_hos_descriptor(mix_sig):\n",
    "    rpks = mix_sig[0:212]\n",
    "    sig = mix_sig[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    sig  = sig[sig < MAX_SENT]\n",
    "    \n",
    "    beats, _ = ecg.extract_heartbeats(signal=sig, rpeaks=rpks, sampling_rate= 300, before=0.25, after=0.25)\n",
    "    #print(beats.shape)\n",
    "    wnlen = len(beats[0])\n",
    "    \n",
    "    n_intervals = 5\n",
    "    beat = np.mean(beats, axis = 0) \n",
    "    lag = int(round( (wnlen)/ n_intervals))\n",
    "    hos_b = np.zeros(( (n_intervals-1) * 2))\n",
    "    for i in range(0, n_intervals-1):\n",
    "        pose = (lag * (i+1))\n",
    "        interval = beat[(pose -int(round(lag/2)) ):(pose + int(round(lag/2)))]\n",
    "        \n",
    "        # Skewness  \n",
    "        hos_b[i] = scipy.stats.skew(interval, axis = 0, bias=True)\n",
    "\n",
    "        if np.isnan(hos_b[i]):\n",
    "            hos_b[i] = 0.0\n",
    "            \n",
    "        # Kurtosis\n",
    "        hos_b[(n_intervals-1) +i] = scipy.stats.kurtosis(interval, 0, False, True)\n",
    "        if np.isnan(hos_b[(n_intervals-1) +i]):\n",
    "            hos_b[(n_intervals-1) +i] = 0.0\n",
    "    return hos_b\n",
    "\n",
    "hos_feats = np.apply_along_axis(compute_hos_descriptor, axis = 1, arr = mix_data2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansrr(rpks):\n",
    "    rpks = rpks[rpks < MAX_SENT]\n",
    "    diff_r = np.ediff1d(rpks)\n",
    "    kmeans = KMeans(n_clusters=2, tol = 40, random_state=0).fit(diff_r.reshape(-1, 1))\n",
    "    labels = kmeans.labels_\n",
    "    #print(labels.shape)\n",
    "    length = len(labels)\n",
    "    len0 = (len(labels[labels < 1]))/length\n",
    "    len1 = (length - len0)/length\n",
    "    cntrs = kmeans.cluster_centers_\n",
    "    return abs(len0*cntrs[0]-len1*cntrs[1])\n",
    "\n",
    "kmeans = np.apply_along_axis(kmeansrr, axis = 1, arr = r_peaks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregression(mix_sig):  # This one should work\n",
    "    rpks = mix_sig[0:212]\n",
    "    sig = mix_sig[212:]\n",
    "    \n",
    "    rpks  = np.array(rpks[rpks < MAX_SENT], dtype = int)\n",
    "    real_sig  = sig[sig < MAX_SENT]\n",
    "    \n",
    "    qrs, _ = ecg.extract_heartbeats(signal=real_sig, rpeaks=rpks, sampling_rate= 300, before=0.056, after=0.056)\n",
    "    stt, _ = ecg.extract_heartbeats(signal=real_sig, rpeaks=rpks, sampling_rate= 300, before=0, after=0.39)\n",
    "    \n",
    "    qrs_avg = np.mean(qrs, axis = 0)\n",
    "    stt_avg = np.mean(stt, axis = 0)\n",
    "    \n",
    "    start_pos = (20/140)*len(stt_avg)\n",
    "    stt_avg = stt_avg[int(start_pos):]\n",
    "    _, qrs_arcoeffs, _, _, _ = levinson_durbin(qrs_avg, nlags=9, isacov=False)\n",
    "    _, stt_arcoeffs, _, _, _ = levinson_durbin(stt_avg, nlags=10, isacov=False)\n",
    "    return np.append(qrs_arcoeffs, stt_arcoeffs)\n",
    "    \n",
    "ar_coeffs = np.apply_along_axis(autoregression, axis = 1, arr = mix_data2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_highfreq(rsig): #baseline wander is not considered noise\n",
    "    sig_ = rsig[rsig < MAX_SENT]\n",
    "    coeffs = wavedec(sig_, 'db6', level=9)\n",
    "    cApprox, *cdetail = coeffs\n",
    "    #new_coeffs = [np.zeros_like(cApprox), cdetail[0], cdetail[1], cdetail[2], cdetail[3], cdetail[4], cdetail[5], cdetail[6], np.zeros_like(cdetail[7]), np.zeros_like(cdetail[8])]\n",
    "    # first and second level detail contain all the noise information detectable for sampling frequency 300 Hz (Nyquist Sampling Theorem)\n",
    "    noise_coeffs = [cdetail[7], cdetail[8]]\n",
    "    return noise_coeffs\n",
    "noise_coeffs = np.apply_along_axis(noise_highfreq, axis = 1, arr = X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4947, 3)\n",
      "(4947, 2)\n",
      "(4947, 6)\n",
      "(4947, 11)\n",
      "(4947, 120)\n",
      "(4947, 1)\n",
      "(4947, 8)\n",
      "(4947, 6)\n",
      "(4947, 2)\n"
     ]
    }
   ],
   "source": [
    "print(rr_stats.shape)\n",
    "print(qrs_stats.shape)\n",
    "print(slop_feats.shape)\n",
    "print(Xtr.shape)\n",
    "print(devvec.shape)\n",
    "print(kmeans.shape)\n",
    "print(hos_feats.shape)\n",
    "print(ar_coeffs.shape)\n",
    "print(noise_coeffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total of 82 feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.concatenate((rr_stats, qrs_stats, slop_feats, Xtr, devvec, hos_feats, kmeans, ar_coeffs, noise_coeffs), axis = 1)\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do same transformation on test X\n",
    "rT_peaks = np.apply_along_axis(getRpeaks, axis = 1, arr = Xt)\n",
    "rrT_stats = np.apply_along_axis(getRRstats, axis = 1, arr = rT_peaks) \n",
    "mixT_data = np.concatenate((rT_peaks, Xt), axis = 1)\n",
    "qrsT_stats = np.apply_along_axis(getQRSwidth, axis = 1, arr =mixT_data)\n",
    "XT_filt = np.apply_along_axis(filtersig, axis = 1, arr = Xt)\n",
    "mixT_data2 = np.concatenate((rT_peaks, XT_filt), axis = 1)\n",
    "slopT_feats = np.apply_along_axis(slopestat, axis = 1, arr = mixT_data2)\n",
    "dwtT_feats = np.apply_along_axis(DWT_beat, axis = 1, arr = mixT_data2)\n",
    "devTvec = np.apply_along_axis(devpreR, axis = 1, arr = mixT_data2)\n",
    "XtrT = sc.transform(dwtT_feats)\n",
    "XtrT = p_var.transform(XtrT)\n",
    "hosT_feats = np.apply_along_axis(compute_hos_descriptor, axis = 1, arr = mixT_data2)   \n",
    "kmeansT = np.apply_along_axis(kmeansrr, axis = 1, arr = rT_peaks) \n",
    "arr_coeffsT = np.apply_along_axis(autoregression, axis = 1, arr = mixT_data2) \n",
    "noiseT_coeffs = np.apply_along_axis(noise_highfreq, axis = 1, arr = Xt) \n",
    "\n",
    "\n",
    "Xtest = np.concatenate((rrT_stats, qrsT_stats, slopT_feats, XtrT, devTvec, hosT_feats, kmeansT, arr_coeffsT, noiseT_coeffs), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseT_coeffs = np.apply_along_axis(noise_highfreq, axis = 1, arr = Xt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3411, 2)\n"
     ]
    }
   ],
   "source": [
    "print(noiseT_coeffs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where you do your XGboost magic\n",
    "# You only need to touch Xtest and Xtrain\n",
    "## Once your code predicts and produces a csv, send me the predicted probabilities, the csv file as well as the optimal parameters (try also using lightgbm using the 'balanced' parameter and catboost using ['Balanced', 'SqrtBalanced'] parameter )\n",
    "# CAUTION: \n",
    "## This code does not predict class 3. Class 3 is predicted by a noise classifier (one versus all). Both are combined using soft voting, i.e. if noise classifier predicts sample 4 is 1 (=class 3) with probability 0.63 but this classifier predicts it is class 2 with probability 0.87 then the final classification will be class 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds012 = [] # This is where you will store your predictions for class 0, 1, and 2 **only**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your code starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinePredictions(preds012, preds3):\n",
    "    final_preds = [0]*len(Xt)\n",
    "    ctr = 0\n",
    "    for idx, el in enumerate(preds3):\n",
    "        if (preds[3] > 0.5):  \n",
    "            final_preds[idx] = 3\n",
    "        else:\n",
    "            final_preds[idx] = preds012[ctr]\n",
    "            ctr += 1\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_preds = combinePredictions(preds012, preds3)\n",
    "dfPredictions = pd.DataFrame()\n",
    "dfPredictions.index.name = \"id\"\n",
    "dfPredictions.to_csv(\"denoisedpreds.csv\", header = ['y'], index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
